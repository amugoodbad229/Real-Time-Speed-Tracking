{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtDQ+ogFgBJUKMlYFubWhR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ---  Usefull ``pip`` commands --- **[OPTIONAL]**\n"
      ],
      "metadata": {
        "id": "WLk-VtrMWX0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swTqH3ieVDIR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Run the command to know what is installed with\n",
        "specific version and what is not on colab\n",
        "'''\n",
        "!pip freeze"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run the command to find out whether specific libraries\n",
        "are installed or not\n",
        "\n",
        "Template:\n",
        "!pip show <library_name> <library_name> ...\n",
        "'''\n",
        "!pip show opencv-python ultralytics supervision"
      ],
      "metadata": {
        "id": "cISp4oFtZ2XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the rest of libraries except ``ultralytics``, ``supervision`` are installed on colab, we need to install ``ultralytics`` and ``supervision``\n",
        "\n",
        "\n",
        "```bash\n",
        "!pip install ultralytics supervision\n",
        "```\n"
      ],
      "metadata": {
        "id": "-SItOcOaf7oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# We'll be using YOLOv12 models for our project\n",
        "---\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2502.12524-b31b1b.svg)](https://arxiv.org/abs/2502.12524)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/sunsmarterjie/yolov12)\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pJ1IS706VL_9z1JyxrWkbJ0wVswHlR1c?usp=sharing)\n",
        "\n",
        "# `YOLOv12`\n",
        "\n",
        "It is a newly proposed attention-centric variant of the YOLO family that focuses on incorporating efficient attention mechanisms into the backbone while preserving real-time performance. Instead of relying heavily on CNN-based architectures like its predecessors, YOLOv12 introduces a simple yet powerful “area attention” module, which strategically partitions the feature map to reduce the quadratic complexity of full self-attention. It also adopts residual efficient layer aggregation networks (R-ELAN) to enhance feature aggregation and training stability, especially for larger models. These innovations, together with refinements such as scaled residual connections and a reduced MLP ratio, enable YOLOv12 to harness the benefits of attention (e.g., better global context modeling) without sacrificing speed.\n",
        "\n",
        "![yolov12-area-attention](https://media.roboflow.com/notebooks/examples/yolov12-area-attention.png)\n",
        "\n",
        "Compared to prior YOLO iterations (e.g., YOLOv10, YOLOv11, and YOLOv8), YOLOv12 achieves higher detection accuracy with competitive or faster inference times across all model scales. Its five sizes—N, S, M, L, and X—range from 2.6M to 59.1M parameters, striking a strong accuracy–speed balance. For instance, the smallest YOLOv12-N surpasses other “nano” models by over 1% mAP with latency around 1.6 ms on a T4 GPU, and the largest YOLOv12-X achieves 55.2% mAP, comfortably outscoring comparable real-time detectors such as RT-DETR and YOLOv11-X . By matching or exceeding state-of-the-art accuracy while remaining fast, YOLOv12 represents a notable step forward for attention-based real-time object detection.\n",
        "\n",
        "![yolov12-metrics](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/yolov12-metrics.png)"
      ],
      "metadata": {
        "id": "2iaXEh2RVkcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitation of ``YOLOv12``\n",
        "\n",
        "A current limitation of ``YOLOv12`` is its reliance on ``FlashAttention`` for optimal speed. ``FlashAttention`` is only supported on relatively modern GPU architectures (NVIDIA Turing, Ampere, Ada Lovelace, or Hopper families) such as ``Tesla T4``, ``RTX 20/30/40-series``, ``A100``, ``H100``, etc.\n",
        "\n",
        "This means older GPUs that lack these architectures cannot fully benefit from YOLOv12’s optimized attention implementation. Users on unsupported hardware would have to fall back to standard attention kernels, losing some speed advantage."
      ],
      "metadata": {
        "id": "ebgeeyq4JyD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU availability\n",
        "\n",
        "**NOTE:** ``YOLOv12`` leverages `FlashAttention` to speed up attention-based computations, but this feature requires an Nvidia GPU built from the above architectures or newer\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that.   \n",
        "\n",
        "In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`.\n",
        "\n",
        "**or** go to 🔽 button below ``share`` option, right beside `connect` -> `Change runtime type` -> `T4 GPU` -> `save`"
      ],
      "metadata": {
        "id": "fMOJk3OeOfuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "JfeDAZsSTgSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# `Project Goal:`\n",
        "\n",
        "**The main goal of this project is to develop a system capable of accurately detecting and displaying the speed of vehicles in real-time from a video feed.**\n",
        "\n"
      ],
      "metadata": {
        "id": "dqe-y-PMlqqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **[MUST RUN]** --> You need to run this to achieve the ``project goal``, some are added with main STEP headings, meaning you need to run all code blocks under it      \n",
        "\n",
        ">**[OPTIONAL]** --> It is optional but important learning steps"
      ],
      "metadata": {
        "id": "gFxANwtooWp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 0: Download the dependencies **[MUST RUN]**"
      ],
      "metadata": {
        "id": "GtKvuDR9hMS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Currently, `YOLOv12` does not have its own `PyPI` package, so we install it directly from GitHub while also adding `flash-attn` (to accelerate attention-based computations via optimized CUDA kernels) and `supervision` (to visualize inference results and benchmark the model’s performance)"
      ],
      "metadata": {
        "id": "qKuh7aphBOVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the official ``YOLOv12`` repository from original authors"
      ],
      "metadata": {
        "id": "-N4z3unXiUWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It will also install all the requirements to run YOLOv12 models\n",
        "# e.g. opencv-python, numpy, ultralytics, etc.\n",
        "!pip install git+https://github.com/sunsmarterjie/yolov12.git"
      ],
      "metadata": {
        "id": "s8aGbs0beJNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ef1e46-1171-4a25-99a9-8afc728f2089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/sunsmarterjie/yolov12.git\n",
            "  Cloning https://github.com/sunsmarterjie/yolov12.git to /tmp/pip-req-build-d1o2xod2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/sunsmarterjie/yolov12.git /tmp/pip-req-build-d1o2xod2\n",
            "  Resolved https://github.com/sunsmarterjie/yolov12.git to commit 2db2baea0f62e896ac85d4c412925a03136ddda7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics==8.3.63)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.3.63) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.3.63) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.63) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics==8.3.63) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.63) (3.0.2)\n",
            "Downloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Building wheels for collected packages: ultralytics\n",
            "  Building wheel for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ultralytics: filename=ultralytics-8.3.63-py3-none-any.whl size=910567 sha256=be6284664e326c82c53520d01e4d72df62c3809871e7d92ffed0e8d7160dbd02\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3gczzwvo/wheels/51/14/51/9f3f73766e89100fb390b031d2290899b2fef7b57d9a74c6dc\n",
            "Successfully built ultralytics\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.63 ultralytics-thop-2.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install ``flash-attn`` without build isolation to ensure it compiles against the currently installed PyTorch and CUDA versions on ``Colab``\n",
        "\n",
        "> ``FlashAttention`` is integrated as a core optimization to enhance the model’s real-time object detection performance."
      ],
      "metadata": {
        "id": "faHkmZ9GBJcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "_6YFB2fPAKlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1109f698-a4cd-4d8f-a534-c7458eda4cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install `supervision` which is **also** a helper library that makes it easy to draw the detection boxes, labels, and tracking lines onto the video frames."
      ],
      "metadata": {
        "id": "SWeWhVI3i2GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "id": "jyscLTmsiwv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d099a6dd-a637-4fb6-f33d-0cd9a297bc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supervision\n",
            "  Downloading supervision-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from supervision) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (1.16.2)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from supervision) (6.0.2)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.12/dist-packages (from supervision) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.12/dist-packages (from supervision) (4.67.1)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.12/dist-packages (from supervision) (4.12.0.88)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.17.0)\n",
            "Downloading supervision-0.26.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: supervision\n",
            "Successfully installed supervision-0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Download Data ---"
      ],
      "metadata": {
        "id": "reNRlTv6v_e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -O means \"output document\" or \"output file\"\n",
        "# gdown is a lightweight Python package designed for downloading files from Google Drive public/sharing links.\n",
        "!pip install gdown\n",
        "\n",
        "# Download the image\n",
        "!gdown https://drive.google.com/uc?id=1SOjAkIsL5Ma0Jn1LrFgbTyGk14KtCwEX -O /content/screenshot.png\n",
        "\n",
        "# Download the video\n",
        "!gdown https://drive.google.com/uc?id=1X6Z7KosRdXVXXMtrbq9TWsAm-UjS3TfB -O /content/input_video.mp4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDGnMI2NwC2g",
        "outputId": "bc572b39-913c-484b-bacd-7cb0463cdd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SOjAkIsL5Ma0Jn1LrFgbTyGk14KtCwEX\n",
            "To: /content/screenshot.png\n",
            "100% 3.13M/3.13M [00:00<00:00, 8.59MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X6Z7KosRdXVXXMtrbq9TWsAm-UjS3TfB\n",
            "To: /content/input_video.mp4\n",
            "100% 12.3M/12.3M [00:00<00:00, 48.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Configuration ---"
      ],
      "metadata": {
        "id": "YnLNus7lvvnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"yolov12l.pt\"\n",
        "SOURCE_VIDEO_PATH = \"input_video.mp4\"\n",
        "TARGET_VIDEO_PATH = \"output_video.mp4\"\n",
        "SOURCE_IMAGE_PATH = \"screenshot.png\""
      ],
      "metadata": {
        "id": "Yjh_9sPEvvP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Import Libraries ---"
      ],
      "metadata": {
        "id": "nJCb-473Qizz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Think of `opencv-python` as the entire toolbox, and `cv2` as the main tool\n",
        "within that box that you grab to start working. It's a common convention\n",
        "in many libraries where the package name for installation might differ\n",
        "slightly from the module name used for importing.\n",
        "\n",
        "Examples:\n",
        "[1] You install Pillow, but import it as PIL\n",
        "    # !pip install Pillow\n",
        "    # import PIL\n",
        "[2] You install scikit-learn, but you import it as sklearn\n",
        "    # !pip install scikit-learn\n",
        "    # import sklearn\n",
        "'''\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# tqdm library is used to show a progress bar\n",
        "from tqdm.notebook import tqdm as notebook_tqdm\n",
        "\n",
        "# deque (double-ended queue) is a data structure that allows for\n",
        "# fast appends and pops from both ends. It is used to maintain a\n",
        "# fixed-length history of positions for each tracked object.\n",
        "from collections import defaultdict, deque"
      ],
      "metadata": {
        "id": "hWW4Rvlrb11a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95dc8bcb-f580-41d0-d1e8-aa67b7273da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/yolov12/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "FlashAttention is not available on this device. Using scaled_dot_product_attention instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draw grid on the image for easy calculations **[OPTIONAL]**\n",
        "\n",
        "> The frame here is a random screenshot of the video. Also the camera is shaky. So you might need to re-adjust the coordinates. Look for ``step 1.2`` for more detailes."
      ],
      "metadata": {
        "id": "6ZC4bsoSf-hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# --- Image Dimensions ---\n",
        "WIDTH = 1920\n",
        "HEIGHT = 1080\n",
        "\n",
        "frame = SOURCE_IMAGE_PATH # A random frame taken from the input video source\n",
        "\n",
        "# --- Load the image ---\n",
        "img = Image.open(frame) # upload the image on colab in folder and paste the name instead of 'i1.png'\n",
        "img_array = np.array(img)\n",
        "\n",
        "# --- Plot Setup ---\n",
        "# You can change the graph size here\n",
        "# This does not change the measurement\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Display the image as the background\n",
        "ax.imshow(img_array, extent=[0, WIDTH, HEIGHT, 0])\n",
        "\n",
        "# 1. Set the X-axis range (0 to WIDTH)\n",
        "ax.set_xlim(0, WIDTH)\n",
        "\n",
        "# 2. Set the Y-axis range (0 to HEIGHT)\n",
        "ax.set_ylim(HEIGHT, 0)\n",
        "\n",
        "# --- Add Grid Lines and Ticks ---\n",
        "# Set major grid lines every 100 pixels\n",
        "major_ticks_x = np.arange(0, WIDTH + 1, 100)\n",
        "major_ticks_y = np.arange(0, HEIGHT + 1, 100)\n",
        "\n",
        "ax.set_xticks(major_ticks_x)\n",
        "ax.set_yticks(major_ticks_y)\n",
        "\n",
        "# Set minor grid lines every 10 pixels for more granularity\n",
        "minor_ticks_x = np.arange(0, WIDTH + 1, 10)\n",
        "minor_ticks_y = np.arange(0, HEIGHT + 1, 10)\n",
        "\n",
        "ax.set_xticks(minor_ticks_x, minor=True)\n",
        "ax.set_yticks(minor_ticks_y, minor=True)\n",
        "\n",
        "# Add the grid lines, making them more transparent\n",
        "ax.grid(which='major', color='white', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "ax.grid(which='minor', color='white', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "\n",
        "# Move the x axis to top\n",
        "ax.xaxis.tick_top()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IZ7NN-wbQep1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: Detect and Track Objects"
      ],
      "metadata": {
        "id": "fEe2a9T1bujy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1.1: Object Detection\n",
        "\n"
      ],
      "metadata": {
        "id": "si5M0_K_AV7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom color setup for specific ``class ID`` **[Optional]**\n",
        "\n",
        "> For this project, I will change the color for cars\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "16itGscyK-YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CAR_CLASS_ID = 2 # see ⭐ section under STEP 1.3\n",
        "\n",
        "# --- Add more classes ---\n",
        "# TRUCK_CLASS_ID = 7\n",
        "# ...\n",
        "\n",
        "# The default color set will be used for all other classes\n",
        "colors = list(sv.ColorPalette.DEFAULT.colors)\n",
        "\n",
        "# Create a dictionary that maps Class ID to the supervision Color object\n",
        "colors[CAR_CLASS_ID] = sv.Color.ROBOFLOW\n",
        "\n",
        "# --- Assign more colors to classes ---\n",
        "# colors[TRUCK_CLASS_ID] = sv.Color.WHITE\n",
        "# ...\n",
        "\n",
        "# Differemt ways to assign colors\n",
        "# [1] color=sv.Color.WHITE\n",
        "# [2] color=sv.Color.from_hex('#ff00ff')\n",
        "# [3] color=sv.Color(r=255, g=255, b=0)\n",
        "\n",
        "# Create the final Color Palette list\n",
        "custom_palette = sv.ColorPalette(colors=colors)"
      ],
      "metadata": {
        "id": "886_PNcy9rRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the ``YOLOv12`` models **[MUST RUN]**"
      ],
      "metadata": {
        "id": "fBR0H7JUVzVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(MODEL_NAME) # Check Configuration Section"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNTvcCq8VsXe",
        "outputId": "dd09c9b8-b0d4-4be0-b53a-778b2307039a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/sunsmarterjie/yolov12/releases/download/turbo/yolov12l.pt to 'yolov12l.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51.4M/51.4M [00:00<00:00, 112MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get ``video`` information to configure the ``output`` file **[MUST RUN]**"
      ],
      "metadata": {
        "id": "j5D1j7hSYOK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)"
      ],
      "metadata": {
        "id": "d98EWGaYWqYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set ``box`` and ``label`` annotation for the objects **[MUST RUN]**\n",
        "\n",
        "> Do not forget to remove `color=custom_palette` if you do not want ``custom color`` for each class object"
      ],
      "metadata": {
        "id": "AwT6g4H9Yej9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "remove `color=custom_palette` if you don't want to assign any custom colors\n",
        "Also, you first need to run the previous block if you want to assign custom colors\n",
        "'''\n",
        "bounding_box_annotator = sv.BoxAnnotator(color=custom_palette) # Creates bounding box on each vehicle\n",
        "\n",
        "# You can add parameters to change font size and color\n",
        "label_annotator = sv.LabelAnnotator(color=custom_palette, text_thickness=2, text_scale=0.7, smart_position=True) # Creates label on each vehicle"
      ],
      "metadata": {
        "id": "_pP8kAXGYdSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Snippet for ``Object Detection`` Only **[OPTIONAL]**"
      ],
      "metadata": {
        "id": "IWqQMsQ4PIJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Why we add ``frame.copy()`` for bounding box , but we add ``annotated_frame`` for label ? It is to ensure that the ``bounding boxes`` are drawn on a fresh copy of the original ``frame``, while the ``labels`` are drawn on the already ``annotated frame`` that includes the ``bounding boxes`` (To understand how label is used, see ``STEP 1.3`` )."
      ],
      "metadata": {
        "id": "z6_6PrbL9qBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# --- Process video frame-by-frame ---\n",
        "with sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:\n",
        "\n",
        "    '''\n",
        "    Using notebook_tqdm to get the progress bar.\n",
        "    If you do not want to use it, then update it to this,\n",
        "    for frame in frame_generator:\n",
        "    '''\n",
        "    for frame in notebook_tqdm(frame_generator, total=video_info.total_frames):\n",
        "        result = model(frame)[0] # yolov12l.pt detects objects\n",
        "        detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "        # Explanation provided in the text block\n",
        "        annotated_frame = bounding_box_annotator.annotate(\n",
        "            scene=frame.copy(),\n",
        "            detections=detections)\n",
        "\n",
        "        # Write the Annotated Frame to the output file\n",
        "        sink.write_frame(frame=annotated_frame)"
      ],
      "metadata": {
        "id": "yzn57MJB-cKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1.2: Source and Target ROI (Region of interest)\n",
        "\n",
        ">Click ▶ [``TLDRAW``](https://www.tldraw.com/f/T6oHe2VW4S5P4fRhE0Aqv?d=v-4969.-631.2962.1407.-3ammlZr97oO5MAsfNS3P). I have provided the coordinate calculation and explanation for both ``SOURCE`` and ``TARGET``.\n",
        "\n",
        ">``SOURCE`` coordinates might be re-adjusted based on the ``frame`` as the camera from the ``SOURCE video`` was shaking a lot. That is why, the ``ROI`` was changing a lot."
      ],
      "metadata": {
        "id": "XuUWGEHUnk50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ``SOURCE`` and ``TARGET`` Coordinates **[MUST RUN]**\n",
        "\n",
        "> ``TARGET`` coordinates are kept realistic based on ``general info``. I might be wrong as I do not the real ``WIDTH`` and ``LENGTH`` of the road from the ``CAM angle``"
      ],
      "metadata": {
        "id": "pPTsuzLrbeI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = np.array([\n",
        "    [780, 160],\n",
        "    [1190, 150],\n",
        "    [2010,1080],\n",
        "    [50, 1080]\n",
        "])\n",
        "\n",
        "# Considered USA Roads\n",
        "TARGET_WIDTH = 20\n",
        "TARGET_HEIGHT = 144\n",
        "\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH - 1, 0],\n",
        "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
        "    [0, TARGET_HEIGHT - 1]\n",
        "])"
      ],
      "metadata": {
        "id": "5O_MhnSPlgJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the ``polygone`` to know whether it is placed correctly or not **[OPTIONAL]**"
      ],
      "metadata": {
        "id": "wrNr504XbqxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ``sv.get_video_frames_generator()`` gives you all frames for the video, ``iter()`` makes it possible to go through them one at a time, and ``next()`` gets the first frame. You can write a loop also."
      ],
      "metadata": {
        "id": "33PEKEwc2wZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
        "frame = iter(frame_generator)\n",
        "frame = next(frame)"
      ],
      "metadata": {
        "id": "X0Hc_oYBsI6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the ``polygon`` (red boundary box) on an image ``frame`` by creating a copy of the ``frame``, drawing the ``polygon`` on it, and then displaying the annotated image."
      ],
      "metadata": {
        "id": "PR0SUmFnRg82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the polygon\n",
        "\n",
        "polygon_frame = frame.copy()\n",
        "polygon_frame = sv.draw_polygon(\n",
        "    scene=polygon_frame,\n",
        "    polygon=SOURCE,\n",
        "    color=sv.Color.RED,\n",
        "    thickness=2\n",
        ")\n",
        "sv.plot_image(polygon_frame)"
      ],
      "metadata": {
        "id": "GXGtWcwB57Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> After proper calculations, we get the following result. I went to a white board to create the ``red`` border and coordinates ``A``, ``B``, ``C`` and ``D``. Click 👉 [``TLDRAW``](https://www.tldraw.com/f/T6oHe2VW4S5P4fRhE0Aqv?d=v-4969.-631.2962.1407.-3ammlZr97oO5MAsfNS3P) to see more details.\n",
        "\n",
        "![SOURCE Coordinates](https://res.cloudinary.com/dd0tkhbsg/image/upload/v1759402593/SOURCE_coordinates_d75rub.png)"
      ],
      "metadata": {
        "id": "ffUmyGJOcGVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1.3: Multi-Object Tracking"
      ],
      "metadata": {
        "id": "YMbTL8XQSKy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiate tracker and filtering **[MUST RUN]**"
      ],
      "metadata": {
        "id": "osPwgBlOa5S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We will be using ``ByteTrack`` algorithm to track the vehicles. The class ``dataset`` is given after the following ``code block.``"
      ],
      "metadata": {
        "id": "q1opzmxpa4ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ByteTrack in Supervision\n",
        "tracker = sv.ByteTrack()\n",
        "\n",
        "# Initiate the polygon zone for filtering\n",
        "polygon = sv.PolygonZone(polygon=SOURCE)"
      ],
      "metadata": {
        "id": "-OB6PzLsXBlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Object Tracking **[OPTIONAL]**"
      ],
      "metadata": {
        "id": "NmhdiPt5bFBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "with sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:\n",
        "    for frame in notebook_tqdm(frame_generator, total=video_info.total_frames):\n",
        "        results = model(frame)[0]  # Use model() for inference\n",
        "\n",
        "        # Detects all objects\n",
        "        detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "        # Starts tracking all detected objects\n",
        "        detections = tracker.update_with_detections(detections=detections)\n",
        "\n",
        "        # Starts filtering based on the polygon (ROI)\n",
        "        # --- START ---\n",
        "        # returns a NumPy array of boolean values, e.g., [False, True, False, ...]\n",
        "        mask = polygon.trigger(detections=detections)\n",
        "        # provides list of IDs of detections that are inside the polygon\n",
        "        detections = detections[mask] # This is called NumPy Slicing\n",
        "        # --- END ---\n",
        "\n",
        "        # Start annotation on a clean frame copy\n",
        "        annotated_frame = frame.copy()\n",
        "\n",
        "        # Draw the Polygon\n",
        "        annotated_frame = sv.draw_polygon(\n",
        "            scene=annotated_frame,\n",
        "            polygon=SOURCE,\n",
        "            color=sv.Color.RED, # Your desired red color\n",
        "            thickness=2\n",
        "        )\n",
        "\n",
        "        # --- Create Labels and Annotate ---\n",
        "        annotated_frame = bounding_box_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=detections\n",
        "        )\n",
        "        # The labels list contains the ID for each vehicle\n",
        "        labels = [f\"{model.names[class_id]} #{tracker_id}\"\n",
        "                  for class_id, tracker_id in zip(detections.class_id, detections.tracker_id)]\n",
        "                  # model.names is a dictionary mapping class_id (e.g., 0, 1) to class name (e.g., 'car', 'person')\n",
        "                  # detections.class_id = [0, 0, 1] and detections.tracker_id = [1, 2, 1]\n",
        "                  # so the zip will produce [(0, 1), (0, 2), (1, 1)]\n",
        "                  # detections is an object that contains the results of the detection model\n",
        "                  # .class_id is a list of class IDs for each detection\n",
        "                  # .tracker_id is a list of unique tracker IDs for each detection\n",
        "                  # iteration 1: class_id=0, tracker_id=1 -> label=\"car #1\"\n",
        "                  # iteration 2: class_id=0, tracker_id=2 -> label=\"car #2\"\n",
        "\n",
        "        # Explanation provided in STEP 1.1 for labels\n",
        "        annotated_frame = label_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=detections,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        # Write the Annotated frame to the output file\n",
        "        sink.write_frame(frame=annotated_frame)"
      ],
      "metadata": {
        "id": "icTJ_P1TSKFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨``Classes`` categorized based on ``COCO`` dataset"
      ],
      "metadata": {
        "id": "1zmGFnzN1kEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "names:\n",
        "  0: person\n",
        "  1: bicycle\n",
        "  2: car\n",
        "  3: motorcycle\n",
        "  4: airplane\n",
        "  5: bus\n",
        "  6: train\n",
        "  7: truck\n",
        "  8: boat\n",
        "  9: traffic light\n",
        "  10: fire hydrant\n",
        "  11: stop sign\n",
        "  12: parking meter\n",
        "  13: bench\n",
        "  14: bird\n",
        "  15: cat\n",
        "  16: dog\n",
        "  17: horse\n",
        "  18: sheep\n",
        "  19: cow\n",
        "  20: elephant\n",
        "  21: bear\n",
        "  22: zebra\n",
        "  23: giraffe\n",
        "  24: backpack\n",
        "  25: umbrella\n",
        "  26: handbag\n",
        "  27: tie\n",
        "  28: suitcase\n",
        "  29: frisbee\n",
        "  30: skis\n",
        "  31: snowboard\n",
        "  32: sports ball\n",
        "  33: kite\n",
        "  34: baseball bat\n",
        "  35: baseball glove\n",
        "  36: skateboard\n",
        "  37: surfboard\n",
        "  38: tennis racket\n",
        "  39: bottle\n",
        "  40: wine glass\n",
        "  41: cup\n",
        "  42: fork\n",
        "  43: knife\n",
        "  44: spoon\n",
        "  45: bowl\n",
        "  46: banana\n",
        "  47: apple\n",
        "  48: sandwich\n",
        "  49: orange\n",
        "  50: broccoli\n",
        "  51: carrot\n",
        "  52: hot dog\n",
        "  53: pizza\n",
        "  54: donut\n",
        "  55: cake\n",
        "  56: chair\n",
        "  57: couch\n",
        "  58: potted plant\n",
        "  59: bed\n",
        "  60: dining table\n",
        "  61: toilet\n",
        "  62: tv\n",
        "  63: laptop\n",
        "  64: mouse\n",
        "  65: remote\n",
        "  66: keyboard\n",
        "  67: cell phone\n",
        "  68: microwave\n",
        "  69: oven\n",
        "  70: toaster\n",
        "  71: sink\n",
        "  72: refrigerator\n",
        "  73: book\n",
        "  74: clock\n",
        "  75: vase\n",
        "  76: scissors\n",
        "  77: teddy bear\n",
        "  78: hair drier\n",
        "  79: toothbrush\n",
        "  '''"
      ],
      "metadata": {
        "id": "0yrAVO7E1PJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2: Transform Perspective\n",
        "\n"
      ],
      "metadata": {
        "id": "TSwYLJaf9rZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Perspective Transformer Class **[MUST RUN]**"
      ],
      "metadata": {
        "id": "H2SftyWXaKKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We need to create ``class`` that will convert the angled perspective to bird's eye view"
      ],
      "metadata": {
        "id": "9Y8CH9_PqrIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViewTransformer:\n",
        "\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
        "        # Creating constructor that takes source and target points as input\n",
        "        # When you create an instance of this class, you need to provide source and target points\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        # Check if the input points array is empty.\n",
        "        # Why is this needed? Because cv2.perspectiveTransform fails on empty arrays\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "\n",
        "        '''\n",
        "        cv2.perspectiveTransform expects points in shape (N, 1, 2). Also, we need\n",
        "        to ensure the points are of type float32, as required by OpenCV functions.\n",
        "        -1 in reshape is a special placeholder that tells NumPy to automatically calculate\n",
        "        the size of that dimension based on the other dimensions and the total number of elements.\n",
        "        e.g., if we have 10 points, reshaping to (-1, 1, 2) will result in (10, 1, 2)\n",
        "\n",
        "        (-1, 1, 2) means \"keep the first dimension (number of points)\n",
        "        # as is, and reshape the last two dimensions to 1 and 2\"\n",
        "        '''\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        # cv2.perspectiveTransform returns points in shape (N, 1, 2), so we need to reshape it back to (N, 2)\n",
        "        return transformed_points.reshape(-1, 2)"
      ],
      "metadata": {
        "id": "hsQJhGBk9ruN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Initiate the ``ViewTransformer`` class"
      ],
      "metadata": {
        "id": "AJ0shoQeomH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)"
      ],
      "metadata": {
        "id": "rtubMYK39y7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object coordinates after perspective transformation **[Optional]**\n",
        "\n",
        "> Running this block, you can generate video with transformed ``x`` and ``y`` coordinates."
      ],
      "metadata": {
        "id": "UjxhKiUGpX1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "with sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:\n",
        "  for frame in notebook_tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "      # 1. Detection, Tracking, and Filtering\n",
        "      results = model(frame)[0]\n",
        "      detections = sv.Detections.from_ultralytics(results)\n",
        "      detections = tracker.update_with_detections(detections=detections)\n",
        "      mask = polygon.trigger(detections=detections)\n",
        "      detections = detections[mask]\n",
        "\n",
        "      # 2. Get the bottom-center point of each bounding box that is touching the ground\n",
        "      points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "\n",
        "      # 3. Transform the points from the image plane to the bird's-eye view (metric space)\n",
        "      points = view_transformer.transform_points(points=points)\n",
        "\n",
        "      # 4. Generate list of Class Name, Tracker ID, and Transformed Coordinates\n",
        "      labels = [f\"{model.names[class_id]} #{tracker_id}\\nx: {point[0]:.0f}, y: {point[1]:.0f}\"\n",
        "                for class_id, tracker_id, point in zip(detections.class_id, detections.tracker_id, points)]\n",
        "\n",
        "      # 5. Draw the ROI Polygon (Background Layer)\n",
        "      annotated_frame = frame.copy()\n",
        "      annotated_frame = sv.draw_polygon(\n",
        "          scene=annotated_frame,\n",
        "          polygon=SOURCE,\n",
        "          color=sv.Color.RED,\n",
        "          thickness=2\n",
        "      )\n",
        "\n",
        "      # 6. Annotate Bounding Boxes (Middle Layer)\n",
        "      annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "\n",
        "      # 7. Annotate Labels (Top Layer)\n",
        "      annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
        "\n",
        "      # 8. Write the frame to the video file\n",
        "      sink.write_frame(frame=annotated_frame)\n"
      ],
      "metadata": {
        "id": "nq1bkanFpd0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: Calculating Speed with Computer Vision **[MUST RUN]**"
      ],
      "metadata": {
        "id": "-AOVs31F7y5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ``defaultdict`` is a **subclass of the built-in dict class**. It overrides one method and adds one writable instance variable. The overridden method is ``__missing__(key)``, which is called by the ``__getitem__()`` method of the dict class when the requested key is not found. In a regular dictionary, if you try to access a key that doesn't exist, it raises a ``KeyError``. However, with ``defaultdict``, if you try to access a ``key`` that doesn't exist, it will automatically ``create`` an entry for that ``key`` using the default factory function provided.\n",
        "\n",
        "Click 👉 [``TLDRAW``](https://www.tldraw.com/f/T6oHe2VW4S5P4fRhE0Aqv?d=v-4969.-631.2962.1407.-3ammlZr97oO5MAsfNS3P) to understand the logic of of the ``code snippet`` of speed.\n"
      ],
      "metadata": {
        "id": "yROfgZx8Bu4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "coordinates varibale created a dictionary that maps each object ID to a deque of its\n",
        "recent (x, y) positions, allowing us to track the movement of each object in the video.\n",
        "'''\n",
        "\n",
        "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\n",
        "# e.g., coordinates = {1: deque([3.4, 3.6, ...], maxlen=30), 2: deque([5.6, 5.8, ...], maxlen=30), ...}\n",
        "# [3.4, 3.6, ...] is the y coordinates we added using .append() for each tracker_id\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "with sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:\n",
        "  for frame in notebook_tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "      # 1. Detection, Tracking, and Filtering\n",
        "      results = model(frame)[0]\n",
        "      detections = sv.Detections.from_ultralytics(results)\n",
        "      detections = tracker.update_with_detections(detections=detections)\n",
        "      mask = polygon.trigger(detections=detections)\n",
        "      detections = detections[mask]\n",
        "\n",
        "      # 2. Get the bottom-center point of each bounding box that is touching the ground\n",
        "      points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
        "\n",
        "      # 3. Transform the points from the image plane to the bird's-eye view (metric space)\n",
        "      points = view_transformer.transform_points(points=points)\n",
        "\n",
        "      # 4. Speed Calculation and Data Storage (Main Goal)\n",
        "\n",
        "      speeds = {} # We store the data so that we can add it in labels\n",
        "      # what does speeds = {} look like in the end?\n",
        "      # speeds = {\n",
        "      #     1: 5.4,  # Speed for object with tracker ID 1\n",
        "      #     2: 7.2   # Speed for object with tracker ID 2\n",
        "      #     ...\n",
        "      #     tracker_id: speed\n",
        "      # }\n",
        "      for tracker_id, [x, y] in zip(detections.tracker_id, points):\n",
        "          # Store the current y-coordinate (distance)\n",
        "          coordinates[tracker_id].append(y)\n",
        "          # Check if enough data is available (e.g., at least 0.5 seconds of frames)\n",
        "          # Kept it 15 frames as video_info.fps is 30, so 30/2 is 15\n",
        "          if len(coordinates[tracker_id]) > video_info.fps / 2:\n",
        "              # Calculate components\n",
        "              coordinate_start = coordinates[tracker_id][-1] # Current Y-coordinate. -1 means the last y coordinate from list\n",
        "              coordinate_end = coordinates[tracker_id][0]    # Oldest Y-coordinate in the deque. 0 means the first y coordinate from list\n",
        "              distance = abs(coordinate_start - coordinate_end) # Just need the positive value\n",
        "              time = len(coordinates[tracker_id]) / video_info.fps\n",
        "              # Calculate Speed (m/s * 3.6 = km/h)\n",
        "              speed = (distance / time) * 3.6\n",
        "              # Store the calculated speed\n",
        "              speeds[tracker_id] = speed\n",
        "\n",
        "      # 5. Generate list of Class Name, Tracker ID, and Transformed Coordinates\n",
        "      labels = [f\"{model.names[class_id]} #{tracker_id}\" +\n",
        "                (f\"\\n{speeds[tracker_id]:.0f} km/h\" if tracker_id in speeds else \"\\n-- km/h\")\n",
        "                for class_id, tracker_id in zip(detections.class_id, detections.tracker_id)]\n",
        "\n",
        "      '''\n",
        "      Optional --> Another way to write Step 5 for more clarity\n",
        "      labels = []\n",
        "      for class_id, tracker_id in zip(detections.class_id, detections.tracker_id):\n",
        "          base = f\"{model.names[class_id]} #{tracker_id}\"\n",
        "          speed_part = f\"\\n{speeds[tracker_id]:.0f} km/h\" if tracker_id in speeds else \"\\n-- km/h\"\n",
        "          labels.append(base + speed_part)\n",
        "      '''\n",
        "\n",
        "      # 6. Draw the ROI Polygon (Background Layer)\n",
        "      annotated_frame = frame.copy()\n",
        "      annotated_frame = sv.draw_polygon(\n",
        "          scene=annotated_frame,\n",
        "          polygon=SOURCE,\n",
        "          color=sv.Color.RED,\n",
        "          thickness=2\n",
        "      )\n",
        "\n",
        "      # 7. Annotate Bounding Boxes (Middle Layer)\n",
        "      annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
        "\n",
        "      # 8. Annotate Labels (Top Layer)\n",
        "      annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
        "\n",
        "      # 9. Write the frame to the video file\n",
        "      sink.write_frame(frame=annotated_frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8edd5dd6994c4b5e99d4a2521bd26f26",
            "be4b15302bff4a7cba39460f18958b08",
            "1377a99c4c394f4f9a365535e1aa7a6d",
            "2286c001148c45e2b131ce6c7cd72be6",
            "8313f3e3694e46f48ca7ea6c5e64b790",
            "be78313c5fb74bc2bb2f1df0b5802eb5",
            "7d6090fe3d01436989c139f6e1212722",
            "43538c60a5f84bc6b46a2218689f7297",
            "5e697ff1c4b0433cbf63c24354c6300f",
            "4a777ba8136b4a5399a4efb298271b00",
            "4c53bf29fb9a414386986da5e439ab13"
          ]
        },
        "id": "n4-cyQefAYZM",
        "outputId": "335278a9-ffe8-4718-eb09-f11c0400e903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/335 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8edd5dd6994c4b5e99d4a2521bd26f26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 9 cars, 1542.3ms\n",
            "Speed: 14.3ms preprocess, 1542.3ms inference, 37.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1337.3ms\n",
            "Speed: 5.6ms preprocess, 1337.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1309.7ms\n",
            "Speed: 6.1ms preprocess, 1309.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1334.4ms\n",
            "Speed: 3.9ms preprocess, 1334.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1335.0ms\n",
            "Speed: 7.9ms preprocess, 1335.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1643.9ms\n",
            "Speed: 4.9ms preprocess, 1643.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2013.3ms\n",
            "Speed: 8.6ms preprocess, 2013.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1341.4ms\n",
            "Speed: 4.9ms preprocess, 1341.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1359.0ms\n",
            "Speed: 5.8ms preprocess, 1359.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1340.5ms\n",
            "Speed: 5.7ms preprocess, 1340.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1339.7ms\n",
            "Speed: 3.7ms preprocess, 1339.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1355.1ms\n",
            "Speed: 5.2ms preprocess, 1355.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1337.4ms\n",
            "Speed: 8.1ms preprocess, 1337.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1331.1ms\n",
            "Speed: 4.9ms preprocess, 1331.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1996.9ms\n",
            "Speed: 8.0ms preprocess, 1996.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1630.7ms\n",
            "Speed: 7.4ms preprocess, 1630.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1365.4ms\n",
            "Speed: 3.8ms preprocess, 1365.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1342.7ms\n",
            "Speed: 5.2ms preprocess, 1342.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1366.6ms\n",
            "Speed: 7.1ms preprocess, 1366.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1353.9ms\n",
            "Speed: 8.6ms preprocess, 1353.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1320.2ms\n",
            "Speed: 4.4ms preprocess, 1320.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1348.6ms\n",
            "Speed: 3.7ms preprocess, 1348.6ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2267.0ms\n",
            "Speed: 8.2ms preprocess, 2267.0ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1841.1ms\n",
            "Speed: 13.6ms preprocess, 1841.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1339.6ms\n",
            "Speed: 6.8ms preprocess, 1339.6ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1397.0ms\n",
            "Speed: 5.7ms preprocess, 1397.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1327.9ms\n",
            "Speed: 5.5ms preprocess, 1327.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1325.3ms\n",
            "Speed: 4.2ms preprocess, 1325.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1370.9ms\n",
            "Speed: 5.3ms preprocess, 1370.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1354.6ms\n",
            "Speed: 7.2ms preprocess, 1354.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1461.3ms\n",
            "Speed: 8.0ms preprocess, 1461.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 2113.3ms\n",
            "Speed: 9.6ms preprocess, 2113.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1447.5ms\n",
            "Speed: 11.4ms preprocess, 1447.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1340.4ms\n",
            "Speed: 6.7ms preprocess, 1340.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1326.4ms\n",
            "Speed: 6.6ms preprocess, 1326.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1340.4ms\n",
            "Speed: 6.1ms preprocess, 1340.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1322.1ms\n",
            "Speed: 6.2ms preprocess, 1322.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1338.0ms\n",
            "Speed: 4.1ms preprocess, 1338.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1680.7ms\n",
            "Speed: 4.2ms preprocess, 1680.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3200.1ms\n",
            "Speed: 6.2ms preprocess, 3200.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1341.0ms\n",
            "Speed: 7.2ms preprocess, 1341.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1328.1ms\n",
            "Speed: 5.5ms preprocess, 1328.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1308.2ms\n",
            "Speed: 7.0ms preprocess, 1308.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1312.9ms\n",
            "Speed: 7.0ms preprocess, 1312.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1319.2ms\n",
            "Speed: 4.9ms preprocess, 1319.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1278.3ms\n",
            "Speed: 3.6ms preprocess, 1278.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1276.6ms\n",
            "Speed: 7.3ms preprocess, 1276.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1907.3ms\n",
            "Speed: 5.7ms preprocess, 1907.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1741.2ms\n",
            "Speed: 10.9ms preprocess, 1741.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1309.9ms\n",
            "Speed: 9.1ms preprocess, 1309.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1361.1ms\n",
            "Speed: 4.8ms preprocess, 1361.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1358.5ms\n",
            "Speed: 5.8ms preprocess, 1358.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1346.9ms\n",
            "Speed: 8.6ms preprocess, 1346.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1322.9ms\n",
            "Speed: 4.8ms preprocess, 1322.9ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1326.6ms\n",
            "Speed: 5.7ms preprocess, 1326.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1474.6ms\n",
            "Speed: 7.1ms preprocess, 1474.6ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2062.5ms\n",
            "Speed: 7.5ms preprocess, 2062.5ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1456.9ms\n",
            "Speed: 6.7ms preprocess, 1456.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1312.4ms\n",
            "Speed: 6.3ms preprocess, 1312.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1365.9ms\n",
            "Speed: 5.0ms preprocess, 1365.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1370.4ms\n",
            "Speed: 8.3ms preprocess, 1370.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1371.3ms\n",
            "Speed: 8.2ms preprocess, 1371.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1318.8ms\n",
            "Speed: 5.7ms preprocess, 1318.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1356.6ms\n",
            "Speed: 6.0ms preprocess, 1356.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1884.0ms\n",
            "Speed: 7.3ms preprocess, 1884.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1784.7ms\n",
            "Speed: 4.5ms preprocess, 1784.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1313.4ms\n",
            "Speed: 6.4ms preprocess, 1313.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1326.8ms\n",
            "Speed: 3.8ms preprocess, 1326.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1333.9ms\n",
            "Speed: 6.4ms preprocess, 1333.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1315.0ms\n",
            "Speed: 5.8ms preprocess, 1315.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1332.5ms\n",
            "Speed: 4.2ms preprocess, 1332.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1330.4ms\n",
            "Speed: 3.9ms preprocess, 1330.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1350.9ms\n",
            "Speed: 7.1ms preprocess, 1350.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2052.6ms\n",
            "Speed: 7.9ms preprocess, 2052.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1528.5ms\n",
            "Speed: 11.7ms preprocess, 1528.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1340.8ms\n",
            "Speed: 6.2ms preprocess, 1340.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1317.7ms\n",
            "Speed: 5.7ms preprocess, 1317.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1359.0ms\n",
            "Speed: 3.8ms preprocess, 1359.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1370.2ms\n",
            "Speed: 5.7ms preprocess, 1370.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1339.7ms\n",
            "Speed: 5.8ms preprocess, 1339.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1324.1ms\n",
            "Speed: 8.1ms preprocess, 1324.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1682.4ms\n",
            "Speed: 6.1ms preprocess, 1682.4ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1948.1ms\n",
            "Speed: 4.3ms preprocess, 1948.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1330.0ms\n",
            "Speed: 7.7ms preprocess, 1330.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1344.3ms\n",
            "Speed: 4.2ms preprocess, 1344.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1321.3ms\n",
            "Speed: 6.4ms preprocess, 1321.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1311.4ms\n",
            "Speed: 6.2ms preprocess, 1311.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1324.1ms\n",
            "Speed: 5.8ms preprocess, 1324.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1357.7ms\n",
            "Speed: 7.5ms preprocess, 1357.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1332.8ms\n",
            "Speed: 4.4ms preprocess, 1332.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1993.4ms\n",
            "Speed: 6.1ms preprocess, 1993.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1626.9ms\n",
            "Speed: 6.6ms preprocess, 1626.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1339.9ms\n",
            "Speed: 5.0ms preprocess, 1339.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1325.0ms\n",
            "Speed: 7.5ms preprocess, 1325.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1319.4ms\n",
            "Speed: 4.7ms preprocess, 1319.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1335.0ms\n",
            "Speed: 5.8ms preprocess, 1335.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1332.7ms\n",
            "Speed: 8.8ms preprocess, 1332.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1379.7ms\n",
            "Speed: 3.6ms preprocess, 1379.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1565.7ms\n",
            "Speed: 4.1ms preprocess, 1565.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2062.5ms\n",
            "Speed: 6.7ms preprocess, 2062.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1346.8ms\n",
            "Speed: 9.6ms preprocess, 1346.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1297.9ms\n",
            "Speed: 6.3ms preprocess, 1297.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1333.7ms\n",
            "Speed: 5.6ms preprocess, 1333.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1317.4ms\n",
            "Speed: 10.0ms preprocess, 1317.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1299.1ms\n",
            "Speed: 4.6ms preprocess, 1299.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1314.9ms\n",
            "Speed: 5.5ms preprocess, 1314.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1296.7ms\n",
            "Speed: 6.6ms preprocess, 1296.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1780.1ms\n",
            "Speed: 4.2ms preprocess, 1780.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1895.9ms\n",
            "Speed: 4.0ms preprocess, 1895.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1313.8ms\n",
            "Speed: 7.5ms preprocess, 1313.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1328.4ms\n",
            "Speed: 7.3ms preprocess, 1328.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1360.9ms\n",
            "Speed: 3.7ms preprocess, 1360.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1335.0ms\n",
            "Speed: 6.1ms preprocess, 1335.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1330.2ms\n",
            "Speed: 8.3ms preprocess, 1330.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 1364.6ms\n",
            "Speed: 6.5ms preprocess, 1364.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 1399.8ms\n",
            "Speed: 5.6ms preprocess, 1399.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 2016.8ms\n",
            "Speed: 5.3ms preprocess, 2016.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1534.3ms\n",
            "Speed: 11.8ms preprocess, 1534.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1729.5ms\n",
            "Speed: 9.2ms preprocess, 1729.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1912.8ms\n",
            "Speed: 9.5ms preprocess, 1912.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1287.4ms\n",
            "Speed: 4.9ms preprocess, 1287.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1317.2ms\n",
            "Speed: 5.5ms preprocess, 1317.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1299.1ms\n",
            "Speed: 5.5ms preprocess, 1299.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1394.7ms\n",
            "Speed: 4.1ms preprocess, 1394.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2004.9ms\n",
            "Speed: 7.2ms preprocess, 2004.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1484.3ms\n",
            "Speed: 5.2ms preprocess, 1484.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1319.1ms\n",
            "Speed: 7.0ms preprocess, 1319.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1330.5ms\n",
            "Speed: 4.2ms preprocess, 1330.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1332.6ms\n",
            "Speed: 9.3ms preprocess, 1332.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1314.3ms\n",
            "Speed: 4.3ms preprocess, 1314.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1332.7ms\n",
            "Speed: 6.0ms preprocess, 1332.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1312.1ms\n",
            "Speed: 7.7ms preprocess, 1312.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1674.0ms\n",
            "Speed: 6.2ms preprocess, 1674.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 1956.2ms\n",
            "Speed: 6.5ms preprocess, 1956.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 1308.0ms\n",
            "Speed: 3.6ms preprocess, 1308.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1333.0ms\n",
            "Speed: 6.8ms preprocess, 1333.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1302.9ms\n",
            "Speed: 9.7ms preprocess, 1302.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1329.8ms\n",
            "Speed: 6.4ms preprocess, 1329.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1305.8ms\n",
            "Speed: 5.4ms preprocess, 1305.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1284.8ms\n",
            "Speed: 7.7ms preprocess, 1284.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1313.0ms\n",
            "Speed: 6.8ms preprocess, 1313.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1872.6ms\n",
            "Speed: 4.3ms preprocess, 1872.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1718.8ms\n",
            "Speed: 9.1ms preprocess, 1718.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1339.0ms\n",
            "Speed: 7.4ms preprocess, 1339.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1356.7ms\n",
            "Speed: 5.1ms preprocess, 1356.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1337.9ms\n",
            "Speed: 8.4ms preprocess, 1337.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1331.5ms\n",
            "Speed: 7.2ms preprocess, 1331.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1339.7ms\n",
            "Speed: 7.6ms preprocess, 1339.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1313.9ms\n",
            "Speed: 5.6ms preprocess, 1313.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1422.9ms\n",
            "Speed: 6.4ms preprocess, 1422.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2032.0ms\n",
            "Speed: 8.8ms preprocess, 2032.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1391.8ms\n",
            "Speed: 9.6ms preprocess, 1391.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1294.3ms\n",
            "Speed: 6.1ms preprocess, 1294.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1288.2ms\n",
            "Speed: 6.8ms preprocess, 1288.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1329.3ms\n",
            "Speed: 7.8ms preprocess, 1329.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1314.7ms\n",
            "Speed: 4.4ms preprocess, 1314.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1306.1ms\n",
            "Speed: 6.9ms preprocess, 1306.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1289.3ms\n",
            "Speed: 3.7ms preprocess, 1289.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1633.0ms\n",
            "Speed: 5.6ms preprocess, 1633.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 2012.3ms\n",
            "Speed: 6.3ms preprocess, 2012.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1332.9ms\n",
            "Speed: 8.5ms preprocess, 1332.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1309.7ms\n",
            "Speed: 4.1ms preprocess, 1309.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1312.2ms\n",
            "Speed: 6.4ms preprocess, 1312.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1294.9ms\n",
            "Speed: 8.3ms preprocess, 1294.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1351.9ms\n",
            "Speed: 5.7ms preprocess, 1351.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1336.1ms\n",
            "Speed: 7.4ms preprocess, 1336.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1317.1ms\n",
            "Speed: 9.6ms preprocess, 1317.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1883.8ms\n",
            "Speed: 6.5ms preprocess, 1883.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1761.5ms\n",
            "Speed: 7.5ms preprocess, 1761.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1311.1ms\n",
            "Speed: 6.4ms preprocess, 1311.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1315.3ms\n",
            "Speed: 7.3ms preprocess, 1315.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1326.0ms\n",
            "Speed: 5.0ms preprocess, 1326.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1315.9ms\n",
            "Speed: 3.8ms preprocess, 1315.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1314.3ms\n",
            "Speed: 4.8ms preprocess, 1314.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1306.5ms\n",
            "Speed: 8.6ms preprocess, 1306.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1409.4ms\n",
            "Speed: 10.0ms preprocess, 1409.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2069.2ms\n",
            "Speed: 14.3ms preprocess, 2069.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1532.5ms\n",
            "Speed: 6.8ms preprocess, 1532.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1323.4ms\n",
            "Speed: 4.8ms preprocess, 1323.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1320.6ms\n",
            "Speed: 8.2ms preprocess, 1320.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1336.8ms\n",
            "Speed: 8.1ms preprocess, 1336.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1386.2ms\n",
            "Speed: 6.3ms preprocess, 1386.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1323.0ms\n",
            "Speed: 4.8ms preprocess, 1323.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1325.1ms\n",
            "Speed: 3.9ms preprocess, 1325.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1796.1ms\n",
            "Speed: 4.8ms preprocess, 1796.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1926.4ms\n",
            "Speed: 10.5ms preprocess, 1926.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1329.9ms\n",
            "Speed: 9.7ms preprocess, 1329.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1338.2ms\n",
            "Speed: 7.0ms preprocess, 1338.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1327.0ms\n",
            "Speed: 13.8ms preprocess, 1327.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1344.0ms\n",
            "Speed: 6.9ms preprocess, 1344.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1327.8ms\n",
            "Speed: 7.7ms preprocess, 1327.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1356.0ms\n",
            "Speed: 11.8ms preprocess, 1356.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1341.0ms\n",
            "Speed: 8.2ms preprocess, 1341.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2044.1ms\n",
            "Speed: 8.8ms preprocess, 2044.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1593.0ms\n",
            "Speed: 10.2ms preprocess, 1593.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1334.6ms\n",
            "Speed: 8.9ms preprocess, 1334.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1311.5ms\n",
            "Speed: 6.7ms preprocess, 1311.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1295.3ms\n",
            "Speed: 13.1ms preprocess, 1295.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1347.9ms\n",
            "Speed: 5.9ms preprocess, 1347.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1327.1ms\n",
            "Speed: 7.1ms preprocess, 1327.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1351.3ms\n",
            "Speed: 5.7ms preprocess, 1351.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1631.0ms\n",
            "Speed: 10.9ms preprocess, 1631.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2052.0ms\n",
            "Speed: 5.7ms preprocess, 2052.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1341.6ms\n",
            "Speed: 6.0ms preprocess, 1341.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1333.8ms\n",
            "Speed: 9.5ms preprocess, 1333.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1347.6ms\n",
            "Speed: 9.8ms preprocess, 1347.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1323.4ms\n",
            "Speed: 7.3ms preprocess, 1323.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1333.7ms\n",
            "Speed: 9.2ms preprocess, 1333.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1321.3ms\n",
            "Speed: 5.7ms preprocess, 1321.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1310.8ms\n",
            "Speed: 6.9ms preprocess, 1310.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1912.7ms\n",
            "Speed: 7.7ms preprocess, 1912.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1728.2ms\n",
            "Speed: 10.9ms preprocess, 1728.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1312.4ms\n",
            "Speed: 7.7ms preprocess, 1312.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1312.0ms\n",
            "Speed: 6.3ms preprocess, 1312.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1332.3ms\n",
            "Speed: 6.6ms preprocess, 1332.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1328.2ms\n",
            "Speed: 6.3ms preprocess, 1328.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1319.4ms\n",
            "Speed: 6.6ms preprocess, 1319.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1396.7ms\n",
            "Speed: 6.3ms preprocess, 1396.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1591.1ms\n",
            "Speed: 6.6ms preprocess, 1591.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2072.7ms\n",
            "Speed: 7.8ms preprocess, 2072.7ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1406.2ms\n",
            "Speed: 5.2ms preprocess, 1406.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1327.1ms\n",
            "Speed: 5.8ms preprocess, 1327.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1354.5ms\n",
            "Speed: 6.6ms preprocess, 1354.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1369.1ms\n",
            "Speed: 7.1ms preprocess, 1369.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1343.0ms\n",
            "Speed: 4.4ms preprocess, 1343.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1321.5ms\n",
            "Speed: 7.7ms preprocess, 1321.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1347.7ms\n",
            "Speed: 10.5ms preprocess, 1347.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1915.5ms\n",
            "Speed: 9.3ms preprocess, 1915.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1734.9ms\n",
            "Speed: 9.1ms preprocess, 1734.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1342.5ms\n",
            "Speed: 7.0ms preprocess, 1342.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1356.5ms\n",
            "Speed: 8.2ms preprocess, 1356.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1330.2ms\n",
            "Speed: 5.7ms preprocess, 1330.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1357.0ms\n",
            "Speed: 5.2ms preprocess, 1357.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1347.3ms\n",
            "Speed: 3.9ms preprocess, 1347.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1316.5ms\n",
            "Speed: 6.8ms preprocess, 1316.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1479.0ms\n",
            "Speed: 3.9ms preprocess, 1479.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2043.1ms\n",
            "Speed: 6.0ms preprocess, 2043.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1396.9ms\n",
            "Speed: 5.2ms preprocess, 1396.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1329.2ms\n",
            "Speed: 7.7ms preprocess, 1329.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1333.3ms\n",
            "Speed: 6.9ms preprocess, 1333.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1383.0ms\n",
            "Speed: 5.7ms preprocess, 1383.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1457.3ms\n",
            "Speed: 4.9ms preprocess, 1457.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 2036.5ms\n",
            "Speed: 6.4ms preprocess, 2036.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1722.3ms\n",
            "Speed: 4.2ms preprocess, 1722.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1970.6ms\n",
            "Speed: 3.9ms preprocess, 1970.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1318.6ms\n",
            "Speed: 7.0ms preprocess, 1318.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1348.8ms\n",
            "Speed: 6.7ms preprocess, 1348.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1337.9ms\n",
            "Speed: 6.1ms preprocess, 1337.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1824.8ms\n",
            "Speed: 4.8ms preprocess, 1824.8ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2085.9ms\n",
            "Speed: 21.6ms preprocess, 2085.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2048.7ms\n",
            "Speed: 16.4ms preprocess, 2048.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3257.8ms\n",
            "Speed: 16.1ms preprocess, 3257.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1607.8ms\n",
            "Speed: 9.4ms preprocess, 1607.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1351.8ms\n",
            "Speed: 6.0ms preprocess, 1351.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1389.5ms\n",
            "Speed: 5.1ms preprocess, 1389.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1377.6ms\n",
            "Speed: 5.8ms preprocess, 1377.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1345.3ms\n",
            "Speed: 7.0ms preprocess, 1345.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1402.1ms\n",
            "Speed: 8.0ms preprocess, 1402.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1343.6ms\n",
            "Speed: 7.8ms preprocess, 1343.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1906.8ms\n",
            "Speed: 6.9ms preprocess, 1906.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1781.6ms\n",
            "Speed: 6.0ms preprocess, 1781.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1339.5ms\n",
            "Speed: 7.6ms preprocess, 1339.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1345.1ms\n",
            "Speed: 6.1ms preprocess, 1345.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1386.9ms\n",
            "Speed: 5.0ms preprocess, 1386.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1370.0ms\n",
            "Speed: 4.2ms preprocess, 1370.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1394.5ms\n",
            "Speed: 4.9ms preprocess, 1394.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1347.8ms\n",
            "Speed: 4.9ms preprocess, 1347.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1554.0ms\n",
            "Speed: 8.2ms preprocess, 1554.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2076.9ms\n",
            "Speed: 9.0ms preprocess, 2076.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1388.3ms\n",
            "Speed: 15.0ms preprocess, 1388.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1321.6ms\n",
            "Speed: 4.1ms preprocess, 1321.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1319.2ms\n",
            "Speed: 6.5ms preprocess, 1319.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1363.7ms\n",
            "Speed: 8.2ms preprocess, 1363.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1310.4ms\n",
            "Speed: 8.1ms preprocess, 1310.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1323.7ms\n",
            "Speed: 5.6ms preprocess, 1323.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1368.8ms\n",
            "Speed: 8.8ms preprocess, 1368.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1862.4ms\n",
            "Speed: 7.0ms preprocess, 1862.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1773.9ms\n",
            "Speed: 9.2ms preprocess, 1773.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1354.6ms\n",
            "Speed: 4.0ms preprocess, 1354.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1361.2ms\n",
            "Speed: 6.4ms preprocess, 1361.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1343.1ms\n",
            "Speed: 4.7ms preprocess, 1343.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1330.6ms\n",
            "Speed: 8.4ms preprocess, 1330.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1361.7ms\n",
            "Speed: 3.9ms preprocess, 1361.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1328.3ms\n",
            "Speed: 5.9ms preprocess, 1328.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1536.5ms\n",
            "Speed: 5.4ms preprocess, 1536.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2068.4ms\n",
            "Speed: 6.1ms preprocess, 2068.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1406.8ms\n",
            "Speed: 7.1ms preprocess, 1406.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1318.1ms\n",
            "Speed: 10.4ms preprocess, 1318.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1335.7ms\n",
            "Speed: 5.7ms preprocess, 1335.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1344.6ms\n",
            "Speed: 3.9ms preprocess, 1344.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1339.1ms\n",
            "Speed: 7.0ms preprocess, 1339.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1337.9ms\n",
            "Speed: 8.2ms preprocess, 1337.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1349.0ms\n",
            "Speed: 8.1ms preprocess, 1349.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1800.0ms\n",
            "Speed: 3.8ms preprocess, 1800.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1873.2ms\n",
            "Speed: 7.8ms preprocess, 1873.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1368.1ms\n",
            "Speed: 8.9ms preprocess, 1368.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1377.0ms\n",
            "Speed: 7.9ms preprocess, 1377.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1342.3ms\n",
            "Speed: 5.8ms preprocess, 1342.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1347.6ms\n",
            "Speed: 5.4ms preprocess, 1347.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1351.4ms\n",
            "Speed: 7.0ms preprocess, 1351.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1388.4ms\n",
            "Speed: 4.1ms preprocess, 1388.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1464.2ms\n",
            "Speed: 5.9ms preprocess, 1464.2ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2079.6ms\n",
            "Speed: 12.3ms preprocess, 2079.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1529.5ms\n",
            "Speed: 8.6ms preprocess, 1529.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1341.9ms\n",
            "Speed: 5.0ms preprocess, 1341.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1337.1ms\n",
            "Speed: 6.1ms preprocess, 1337.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1320.2ms\n",
            "Speed: 6.7ms preprocess, 1320.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1362.8ms\n",
            "Speed: 5.5ms preprocess, 1362.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1348.1ms\n",
            "Speed: 8.5ms preprocess, 1348.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1325.6ms\n",
            "Speed: 5.6ms preprocess, 1325.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1812.2ms\n",
            "Speed: 6.6ms preprocess, 1812.2ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1891.3ms\n",
            "Speed: 8.9ms preprocess, 1891.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1335.2ms\n",
            "Speed: 4.9ms preprocess, 1335.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1330.2ms\n",
            "Speed: 6.2ms preprocess, 1330.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1378.3ms\n",
            "Speed: 3.9ms preprocess, 1378.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1318.9ms\n",
            "Speed: 5.6ms preprocess, 1318.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1302.1ms\n",
            "Speed: 5.7ms preprocess, 1302.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1354.3ms\n",
            "Speed: 3.7ms preprocess, 1354.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 1325.3ms\n",
            "Speed: 5.5ms preprocess, 1325.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2043.3ms\n",
            "Speed: 5.7ms preprocess, 2043.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1648.0ms\n",
            "Speed: 12.1ms preprocess, 1648.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1311.1ms\n",
            "Speed: 6.2ms preprocess, 1311.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1317.4ms\n",
            "Speed: 4.1ms preprocess, 1317.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1340.2ms\n",
            "Speed: 5.6ms preprocess, 1340.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1326.9ms\n",
            "Speed: 7.9ms preprocess, 1326.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1323.2ms\n",
            "Speed: 7.0ms preprocess, 1323.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1336.4ms\n",
            "Speed: 5.2ms preprocess, 1336.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1569.8ms\n",
            "Speed: 8.3ms preprocess, 1569.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2070.6ms\n",
            "Speed: 14.3ms preprocess, 2070.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1396.6ms\n",
            "Speed: 5.7ms preprocess, 1396.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1417.9ms\n",
            "Speed: 4.4ms preprocess, 1417.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1372.0ms\n",
            "Speed: 4.1ms preprocess, 1372.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1338.3ms\n",
            "Speed: 4.4ms preprocess, 1338.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1371.7ms\n",
            "Speed: 4.0ms preprocess, 1371.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1330.8ms\n",
            "Speed: 4.2ms preprocess, 1330.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank you for learning with me 🥰\n",
        "\n",
        "# Follow my [``Github``](https://github.com/amugoodbad229) for more ``tutorials`` like this and ``Tech News`` that will kinda shock you and don't forget to give a ⭐ on the ``projects`` I have created for ``educational purposes``.\n",
        "\n",
        "# Have a great day! 🥳"
      ],
      "metadata": {
        "id": "-EIWhs-cAG6P"
      }
    }
  ]
}
